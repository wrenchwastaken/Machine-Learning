{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPF7ufXkNxuuv7UpogAOXA2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"xLpPAbak--y1","executionInfo":{"status":"ok","timestamp":1690287865909,"user_tz":-330,"elapsed":1544,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}}},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","source":["# Question 1\n","* Import the data from the file Human_Activity_Recognition_Using_Smartphones_Data.csv and examine the shape and data types. For the data types, there will be too many to list each column separately. Rather, aggregate the types by count.\n","* Determine if the float columns need to be scaled."],"metadata":{"id":"zIDlRZqNAkFl"}},{"cell_type":"code","source":["data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')"],"metadata":{"id":"1fWI13GjAs6T","executionInfo":{"status":"ok","timestamp":1690287936172,"user_tz":-330,"elapsed":7080,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data.shape\n","data.dtypes.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1w4qGiNAvKN","executionInfo":{"status":"ok","timestamp":1690287958501,"user_tz":-330,"elapsed":5,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}},"outputId":"38b93215-a1e2-4629-bfa3-0f42f86d5cbf"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["float64    561\n","object       1\n","dtype: int64"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Mask to select float columns\n","float_columns = (data.dtypes == np.float)\n","\n","# Verify that the maximum of all float columns is 1.0\n","print( (data.loc[:,float_columns].max()==1.0).all() )\n","\n","# Verify that the minimum of all float columns is -1.0\n","print( (data.loc[:,float_columns].min()==-1.0).all() )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4HJB-5HA0Or","executionInfo":{"status":"ok","timestamp":1690287990741,"user_tz":-330,"elapsed":4,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}},"outputId":"62a27c96-edf8-4b94-96de-52fb26bddd3f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","True\n"]}]},{"cell_type":"markdown","source":["# Question 2\n","\n","* Integer encode the activities.\n","* Split the data into train and test data sets. Decide if the data will be stratified or not during the train/test split."],"metadata":{"id":"BZ9z5snfBAfS"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","data['Activity'] = le.fit_transform(data['Activity'])\n","\n","le.classes_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XARN2aqSA-Ed","executionInfo":{"status":"ok","timestamp":1690288048925,"user_tz":-330,"elapsed":1128,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}},"outputId":"50ddc1ae-23c1-4924-8eb7-657995d39185"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS',\n","       'WALKING_UPSTAIRS'], dtype=object)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["data.Activity.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_e9Jwb_hBMF7","executionInfo":{"status":"ok","timestamp":1690288055537,"user_tz":-330,"elapsed":5,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}},"outputId":"5d0857de-a131-4609-c053-91aa38bfaf4d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 1, 0, 3, 4, 5])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["* NOTE: We are about to create training and test sets from data. On those datasets, we are going to run grid searches over many choices of parameters. This can take some time. In order to shorten the grid search time, feel free to downsample data and create X_train, X_test, y_train, y_test from the downsampled dataset.\n","\n","* Now split the data into train and test data sets. A stratified split was not used here. If there are issues with any of the error metrics on the test set, it can be a good idea to start model fitting over using a stratified split. Boosting is a pretty powerful model, though, so it may not be necessary in this case."],"metadata":{"id":"2njW8vNhBTIw"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Alternatively, we could stratify the categories in the split, as was done previously\n","feature_columns = [x for x in data.columns if x != 'Activity']\n","\n","X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data['Activity'],\n","                 test_size=0.3, random_state=42)"],"metadata":{"id":"B04O_83ABWRJ","executionInfo":{"status":"ok","timestamp":1690288115116,"user_tz":-330,"elapsed":3,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSVQ9QIZBcAM","executionInfo":{"status":"ok","timestamp":1690288126609,"user_tz":-330,"elapsed":442,"user":{"displayName":"The First Wrenxh","userId":"12890727307911892006"}},"outputId":"99c607ce-ce87-449a-aca3-77740483793d"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((7209, 561), (7209,), (3090, 561), (3090,))"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Question 3\n","\n","* Fit gradient boosted tree models with all parameters set to their defaults with the following tree numbers (n_estimators = [15, 25, 50, 100, 200, 400]) and evaluate the accuracy on the test data for each of these models.\n","* Plot the accuracy as a function of estimator number\n","\n","Note: there is no out-of-bag error for boosted models. And the warm_flag=True setting has a bug in the gradient boosted model, so don't use it. Simply create the model inside the for loop and set the number of estimators at this time. This will make the fitting take a little longer. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively."],"metadata":{"id":"cdokH3N1B9uu"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","error_list = list()\n","\n","# Iterate through various possibilities for number of trees\n","tree_list = [15, 25, 50, 100, 200, 400]\n","for n_trees in tree_list:\n","\n","    # Initialize the gradient boost classifier\n","    GBC = GradientBoostingClassifier(n_estimators=n_trees, random_state=42)\n","\n","    # Fit the model\n","    print(f'Fitting model with {n_trees} trees')\n","    GBC.fit(X_train.values, y_train.values)\n","    y_pred = GBC.predict(X_test)\n","\n","    # Get the error\n","    error = 1.0 - accuracy_score(y_test, y_pred)\n","\n","    # Store it\n","    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n","\n","error_df = pd.concat(error_list, axis=1).T.set_index('n_trees')\n","\n","error_df"],"metadata":{"id":"LCo3uqA0CPHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_context('talk')\n","sns.set_style('white')\n","\n","# Create the plot\n","ax = error_df.plot(marker='o', figsize=(12, 8), linewidth=5)\n","\n","# Set parameters\n","ax.set(xlabel='Number of Trees', ylabel='Error')\n","ax.set_xlim(0, max(error_df.index)*1.1);"],"metadata":{"id":"ITwQ-xhOcM2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 4\n","\n","* Using a grid search with cross-validation, fit a new gradient boosted classifier with the same list of estimators as question 3. Also try varying the learning rates (0.1, 0.01, 0.001, etc.), the subsampling value (1.0 or 0.5), and the number of maximum features (1, 2, etc.).\n","* Examine the parameters of the best fit model.\n","* Calculate relevant error metrics on this model and examine the confusion matrix."],"metadata":{"id":"Q_-UywX1cPVk"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# The parameters to be fit\n","param_grid = {'n_estimators': tree_list,\n","              'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n","              'subsample': [1.0, 0.5],\n","              'max_features': [1, 2, 3, 4]}\n","\n","# The grid search object\n","GV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42),\n","                      param_grid=param_grid,\n","                      scoring='accuracy',\n","                      n_jobs=-1)\n","\n","# Do the grid search\n","GV_GBC = GV_GBC.fit(X_train, y_train)\n","# The best model\n","GV_GBC.best_estimator_"],"metadata":{"id":"6eZV6LrQcaeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","y_pred = GV_GBC.predict(X_test)\n","print(classification_report(y_pred, y_test))\n"],"metadata":{"id":"d6rNW914cjhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","sns.set_context('talk')\n","cm = confusion_matrix(y_test, y_pred)\n","ax = sns.heatmap(cm, annot=True, fmt='d')"],"metadata":{"id":"lBjFqlicclt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 5\n","* Create an AdaBoost model and fit it using grid search, much like question 4.\n","* Try a range of estimators between 100 and 200.\n","Compare the errors from AdaBoost to those from the GradientBoostedClassifier."],"metadata":{"id":"BhdP-wSccudN"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","ABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n","\n","param_grid = {'n_estimators': [100, 150, 200],\n","              'learning_rate': [0.01, 0.001]}\n","\n","GV_ABC = GridSearchCV(ABC,\n","                      param_grid=param_grid,\n","                      scoring='accuracy',\n","                      n_jobs=-1)\n","\n","GV_ABC = GV_ABC.fit(X_train, y_train)"],"metadata":{"id":"c5riZdoec8Mp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The best model\n","GV_ABC.best_estimator_"],"metadata":{"id":"kAJqHlAec-VG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = GV_ABC.predict(X_test)\n","print(classification_report(y_pred, y_test))\n","sns.set_context('talk')\n","cm = confusion_matrix(y_test, y_pred)\n","ax = sns.heatmap(cm, annot=True, fmt='d')\n","### END SOLUTION"],"metadata":{"id":"9S0KkYHAdAcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 6\n","\n","* Fit a logistic regression model with regularization.\n","* Using VotingClassifier, fit the logistic regression model along with either the GratientBoostedClassifier or the AdaBoost model (or both) from questions 4 and 5.\n","* Determine the error as before and compare the results to the appropriate gradient boosted model(s).\n","* Plot the confusion matrix for the best model created in this set of exercises."],"metadata":{"id":"qqAB_NZvdD1s"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","# L2 regularized logistic regression\n","LR_L2 = LogisticRegression(penalty='l2', max_iter=500, solver='saga').fit(X_train, y_train)"],"metadata":{"id":"Nq9pNB4tdOPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = LR_L2.predict(X_test)\n","print(classification_report(y_pred, y_test))"],"metadata":{"id":"8gHncXjWdQh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_context('talk')\n","cm = confusion_matrix(y_test, y_pred)\n","ax = sns.heatmap(cm, annot=True, fmt='d')"],"metadata":{"id":"9IzWs76XdSdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import VotingClassifier\n","\n","# The combined model--logistic regression and gradient boosted trees\n","estimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\n","\n","# Though it wasn't done here, it is often desirable to train\n","# this model using an additional hold-out data set and/or with cross validation\n","VC = VotingClassifier(estimators, voting='soft')\n","VC = VC.fit(X_train, y_train)"],"metadata":{"id":"pnbKBACFdUMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Performance for the voting classifier should improve relative to either logistic regression or gradient boosted trees alone.\n","#However, the fact that logistic regression does almost as well as gradient boosted trees is an important reminder to try the simplest model first.\n","#In some cases, its performance will be good enough.\n","y_pred = VC.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","sns.set_context('talk')\n","cm = confusion_matrix(y_test, y_pred)\n","ax = sns.heatmap(cm, annot=True, fmt='d')"],"metadata":{"id":"1SXRo2zydV3w"},"execution_count":null,"outputs":[]}]}